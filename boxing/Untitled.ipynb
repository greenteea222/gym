{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b43cc7a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:335: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0 True False 120\n",
      "0.0 True False 23\n",
      "0.0 True False 21\n",
      "0.0 True False 23\n",
      "0.0 True False 23\n",
      "mean reward = 0.4\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import random\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from wrapper import AtariWrapper\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "model = tf.keras.models.load_model(\"./breakout_20240315_215140/model_20\")\n",
    "\n",
    "noop_max = 30\n",
    "frame_skip = 4\n",
    "screen_size = 84\n",
    "terminal_on_life_loss = True\n",
    "clip_reward = False\n",
    "action_repeat_probability = 0.0\n",
    "normalize = True\n",
    "\n",
    "# init env and model struct\n",
    "env = gym.make(\"BreakoutNoFrameskip-v4\", render_mode=\"human\")\n",
    "env = AtariWrapper(env, noop_max=noop_max, frame_skip=frame_skip, screen_size=screen_size,\n",
    "                   terminal_on_life_loss=terminal_on_life_loss, clip_reward=clip_reward,\n",
    "                   action_repeat_probability=action_repeat_probability, normalize=normalize)\n",
    "\n",
    "print(env.action_space)\n",
    "\n",
    "\n",
    "state = tf.convert_to_tensor(env.reset()[0])\n",
    "state = tf.stack([state, state, state, state], axis=-1)\n",
    "state = tf.expand_dims(state, 0)\n",
    "\n",
    "total_reward = []\n",
    "reward_ = 0\n",
    "for _ in range(5):\n",
    "    done = False\n",
    "    truncated = False\n",
    "    cnt = 0\n",
    "    while not (done or truncated):\n",
    "        cnt += 1\n",
    "        # traj_info = model(state)\n",
    "        # action = traj_info['a'].numpy()\n",
    "        # next_state, reward, done, truncated, info = env.step(action[0])  # 更新状态信息\n",
    "        action = env.action_space.sample() # 采取一个动作\n",
    "        next_state, reward, done, truncated, info = env.step(action) # 更新状态信息\n",
    "\n",
    "        reward_ += reward\n",
    "\n",
    "        next_state = tf.expand_dims(next_state, 0)\n",
    "        next_state = tf.stack([next_state, state[:, :, :, 0], state[:, :, :, 1], state[:, :, :, 2]], axis=-1)\n",
    "\n",
    "#         print(tf.reduce_mean(next_state - state), action)\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        env.render()\n",
    "        #         print(reward)\n",
    "\n",
    "        if done or truncated:\n",
    "            state = tf.convert_to_tensor(env.reset()[0])\n",
    "            state = tf.stack([state, state, state, state], axis=-1)\n",
    "            state = tf.expand_dims(state, 0)\n",
    "            total_reward.append(reward_)\n",
    "\n",
    "            print(reward_, done, truncated, cnt)\n",
    "\n",
    "            reward_ = 0\n",
    "            break\n",
    "\n",
    "average_reward = np.mean(total_reward)\n",
    "\n",
    "print(f\"mean reward = {average_reward}\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49defbe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noop_max = 30, frame_skip = 4, screen_size = 84, terminal_on_life_loss = True, clip_reward = True, action_repeat_probability = 0.0, normalize = True \n",
      "\n",
      "start training + \n",
      "GAMA = 0.99, GAE_LAMBDA = 0.95, EPISODE_LENGTH = 4096, TRAIN_BATCH_SIZE = 128, TRAIN_EPOCHS = 2, LEARNING_RATE = 0.00025, CLIP_VALUE = 0.2, ENTROPY_BETA = 0.03, NUM_ACTIONS = 18, CRITIC_BETA = 0.5, GRAD_CLIP = 5.0\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_probability as tfp\n",
    "import time\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import os\n",
    "from utils import Transition, ReplayMemory, VideoRecorder\n",
    "from wrapper import AtariWrapper\n",
    "\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "# get time\n",
    "now = datetime.now()\n",
    "time_format = \"%Y%m%d_%H%M%S\"\n",
    "current_time = now.strftime(time_format)\n",
    "\n",
    "if not os.path.exists(\"./boxing_{}\".format(current_time)):\n",
    "    os.makedirs(\"./boxing_{}\".format(current_time))\n",
    "log_file = \"./boxing_{}/train_log\".format(current_time)\n",
    "\n",
    "noop_max = 30\n",
    "frame_skip = 4\n",
    "screen_size = 84\n",
    "terminal_on_life_loss = True\n",
    "clip_reward = True\n",
    "action_repeat_probability = 0.0\n",
    "normalize = True\n",
    "\n",
    "# init env and model struct\n",
    "env = gym.make('BoxingNoFrameskip-v4')\n",
    "env = AtariWrapper(env, noop_max = noop_max, frame_skip = frame_skip, screen_size = screen_size, terminal_on_life_loss = terminal_on_life_loss, clip_reward = clip_reward, action_repeat_probability = action_repeat_probability, normalize = normalize)\n",
    "\n",
    "env_str = (f\"noop_max = {noop_max}, frame_skip = {frame_skip}, screen_size = {screen_size}, \"\n",
    "                  f\"terminal_on_life_loss = {terminal_on_life_loss}, clip_reward = {clip_reward}, action_repeat_probability = {action_repeat_probability}, \"\n",
    "                  f\"normalize = {normalize} \\n\")\n",
    "\n",
    "print(env_str)\n",
    "with open(log_file, mode='a') as filename:\n",
    "    filename.write(env_str + '\\n')\n",
    "\n",
    "## training params\n",
    "EPOCHS = 1000\n",
    "EPISODE_LENGTH = 4096\n",
    "TRAIN_BATCH_SIZE = 128\n",
    "TRAIN_EPOCHS = 2\n",
    "LEARNING_RATE = 0.00025\n",
    "GRAD_CLIP = 5.0\n",
    "\n",
    "## general params\n",
    "NUM_ACTIONS = env.action_space.n\n",
    "CRITIC_BETA = 0.5\n",
    "\n",
    "## ppo params\n",
    "CLIP_VALUE = 0.2\n",
    "ENTROPY_BETA = 0.03\n",
    "\n",
    "# gae params\n",
    "USE_GAE = True\n",
    "GAMA = 0.99\n",
    "GAE_LAMBDA = 0.95\n",
    "\n",
    "\n",
    "parameters_str = \"start training + \\n\" + \\\n",
    "                 (f\"GAMA = {GAMA}, GAE_LAMBDA = {GAE_LAMBDA}, EPISODE_LENGTH = {EPISODE_LENGTH}, \"\n",
    "                  f\"TRAIN_BATCH_SIZE = {TRAIN_BATCH_SIZE}, TRAIN_EPOCHS = {TRAIN_EPOCHS}, LEARNING_RATE = {LEARNING_RATE}, \"\n",
    "                  f\"CLIP_VALUE = {CLIP_VALUE}, ENTROPY_BETA = {ENTROPY_BETA}, NUM_ACTIONS = {NUM_ACTIONS}, \"\n",
    "                  f\"CRITIC_BETA = {CRITIC_BETA}, GRAD_CLIP = {GRAD_CLIP}\")\n",
    "\n",
    "print(parameters_str)\n",
    "with open(log_file, mode='a') as filename:\n",
    "    filename.write(parameters_str + '\\n')\n",
    "\n",
    "\n",
    "class Logger(object):\n",
    "    def __init__(self, output_file):\n",
    "        self.terminal = sys.stdout\n",
    "        self.log = open(output_file, \"a\")\n",
    "\n",
    "    def write(self, message):\n",
    "        self.terminal.write(message)\n",
    "        self.terminal.flush()\n",
    "        self.log.write(message)\n",
    "        self.log.flush()\n",
    "\n",
    "    def flush(self):\n",
    "        self.terminal.flush()\n",
    "        self.log.flush()\n",
    "\n",
    "    def close(self):\n",
    "        self.log.close()\n",
    "\n",
    "\n",
    "# model class\n",
    "class ActorModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = tf.keras.layers.Conv2D(32, kernel_size=8, strides=4, activation='relu')\n",
    "        self.conv2 = tf.keras.layers.Conv2D(64, kernel_size=4, strides=3, activation='relu')\n",
    "        self.conv3 = tf.keras.layers.Conv2D(64, kernel_size=3, strides=1, activation='relu')\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.fc = tf.keras.layers.Dense(512, activation='relu')\n",
    "        self.actor = layers.Dense(NUM_ACTIONS, activation='softmax')\n",
    "        self.critic = layers.Dense(1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = tf.divide(state, 255)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc(x)\n",
    "        return self.actor(x), self.critic(x)\n",
    "\n",
    "    def act(self, state, action=None):\n",
    "        prob, v = self.forward(state)\n",
    "        dist = tfp.distributions.Categorical(probs=prob)\n",
    "        if action is None:\n",
    "            action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        entropy = dist.entropy()\n",
    "\n",
    "        return {'a': action,\n",
    "                'log_pi_a': log_prob,\n",
    "                'ent': entropy,\n",
    "                'v': tf.squeeze(v)}  # 去除维度为1的维度\n",
    "\n",
    "    def call(self, state, action = None):\n",
    "        return self.act(state, action=action)\n",
    "\n",
    "# ppo struct\n",
    "class PPO:\n",
    "    def __init__(self):\n",
    "        self.model = ActorModel()\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE, clipvalue=GRAD_CLIP)\n",
    "        # self.optimizer = tf.keras.optimizers.Adamax(learning_rate=LEARNING_RATE, clipvalue=GRAD_CLIP)\n",
    "\n",
    "    def train(self, states, actions, advantages, log_probs_old, returns, values, clip_val, entropy_beta, idxes):\n",
    "        total_loss = 0.0\n",
    "        critic_loss = 0.0\n",
    "        actor_loss = 0.0\n",
    "        entropy_loss = 0.0\n",
    "\n",
    "        ## get training sam data\n",
    "        states = [states[i] for i in idxes]\n",
    "        actions = [actions[i] for i in idxes]\n",
    "        advantages = [advantages[i] for i in idxes]\n",
    "        log_probs_old = [log_probs_old[i] for i in idxes]\n",
    "        returns = [returns[i] for i in idxes]\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            states = tf.expand_dims(states, 1)\n",
    "\n",
    "            traj_info = self.model(states, actions)\n",
    "\n",
    "            ## actor loss\n",
    "            ratio = tf.exp(traj_info['log_pi_a'] - log_probs_old)\n",
    "            advantages = tf.convert_to_tensor(advantages, dtype=tf.float32)\n",
    "            advantages = tf.expand_dims(advantages, -1)\n",
    "            surr1 = tf.multiply(ratio , advantages)\n",
    "            surr2 = tf.multiply(tf.clip_by_value(ratio, 1 - clip_val, 1 + clip_val) , advantages)\n",
    "            actor_loss = -tf.reduce_mean(tf.minimum(surr1, surr2))\n",
    "\n",
    "            # critic loss\n",
    "            critic_loss = tf.keras.losses.mean_squared_error(returns, traj_info['v'])\n",
    "            critic_loss = tf.reduce_mean(critic_loss)\n",
    "\n",
    "            # entropy loss\n",
    "            entropy_loss = tf.reduce_mean(traj_info['ent'])\n",
    "\n",
    "            total_loss = actor_loss + CRITIC_BETA * critic_loss - entropy_beta * entropy_loss\n",
    "\n",
    "            # backprop\n",
    "            grads = tape.gradient(total_loss, self.model.trainable_variables)\n",
    "            variables = self.model.trainable_variables\n",
    "            self.optimizer.apply_gradients(zip(grads, variables))\n",
    "            actor_grads_norm = tf.linalg.global_norm(grads)\n",
    "\n",
    "        return critic_loss, actor_loss, entropy_loss, total_loss, actor_grads_norm\n",
    "\n",
    "def gae(rewards, dones, values, epsoide_length, vals_last):\n",
    "    returns = np.zeros_like(rewards)\n",
    "    advantages = np.zeros_like(rewards)\n",
    "\n",
    "    if not USE_GAE:\n",
    "        for t in reversed(range(epsoide_length)):\n",
    "            if t == epsoide_length - 1:\n",
    "                returns[t] = rewards[t] + GAMA * (1 - dones[t]) * vals_last\n",
    "            else:\n",
    "                returns[t] = rewards[t] + GAMA * (1 - dones[t]) * returns[t + 1]\n",
    "            advantages[t] = returns[t] - values[t]\n",
    "    else:\n",
    "        for t in reversed(range(epsoide_length)):\n",
    "            if t == epsoide_length - 1:\n",
    "                returns[t] = rewards[t] + GAMA * (1 - dones[t]) * vals_last\n",
    "                td_error = returns[t] - values[t]\n",
    "            else:\n",
    "                returns[t] = rewards[t] + GAMA * (1 - dones[t]) * returns[t + 1]\n",
    "                td_error = rewards[t] + GAMA * (1 - dones[t]) * values[t + 1] - values[t]\n",
    "            advantages[t] = advantages[t] * GAE_LAMBDA * GAMA * (1 - dones[t]) + td_error\n",
    "    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-10)\n",
    "    return returns, advantages\n",
    "\n",
    "def save_model(ppo, epoch):\n",
    "    actor_save_path = f\"./boxing_{current_time}/model_{epoch}\"\n",
    "    tf.keras.models.save_model(ppo.model, actor_save_path)\n",
    "\n",
    "def evaluate_model(ppo, env, num_episodes=5):\n",
    "    # state = env.reset()\n",
    "    # state = np.array([state[0]])\n",
    "\n",
    "    state = tf.convert_to_tensor(env.reset()[0])\n",
    "    state = tf.stack([state, state, state, state], axis=-1)\n",
    "    state = tf.expand_dims(state, 0)\n",
    "\n",
    "    total_reward = []\n",
    "    reward_ = 0\n",
    "    for _ in range(num_episodes):\n",
    "        done = False\n",
    "        truncated = False\n",
    "        while not (done or truncated):\n",
    "            traj_info = ppo.model(state)\n",
    "            action = traj_info['a'].numpy()\n",
    "            next_state, reward, done, truncated, info = env.step(action[0])  # 更新状态信息\n",
    "            reward_ += reward\n",
    "\n",
    "            next_state = tf.expand_dims(next_state, 0)\n",
    "            next_state = tf.stack([next_state, state[:, :, :, 0], state[:, :, :, 1], state[:, :, :, 2]], axis=-1)\n",
    "            state = next_state\n",
    "\n",
    "            if done or truncated:\n",
    "                state = tf.convert_to_tensor(env.reset()[0])\n",
    "                state = tf.stack([state, state, state, state], axis=-1)\n",
    "                state = tf.expand_dims(state, 0)\n",
    "                total_reward.append(reward_)\n",
    "                reward_ = 0\n",
    "                break\n",
    "\n",
    "    average_reward = np.mean(total_reward)\n",
    "    return average_reward\n",
    "\n",
    "# def random_sample(inds, minibatch_size):\n",
    "#     all_batches = [np.arange(i * minibatch_size, min((i + 1) * minibatch_size, inds)) for i in\n",
    "#                    range((inds + minibatch_size - 1) // minibatch_size)]\n",
    "#\n",
    "#     perm_inds = np.random.permutation(len(all_batches))\n",
    "#\n",
    "#     for ind in perm_inds:\n",
    "#         yield tf.convert_to_tensor(all_batches[ind], dtype=tf.int64)\n",
    "\n",
    "def random_sample(inds, minibatch_size):\n",
    "    inds = np.random.permutation(inds)\n",
    "    num_full_batches = len(inds) // minibatch_size\n",
    "\n",
    "    for i in range(num_full_batches):\n",
    "        batch_inds = inds[i * minibatch_size: (i + 1) * minibatch_size]\n",
    "        yield tf.convert_to_tensor(batch_inds, dtype=tf.int64)\n",
    "\n",
    "    r = len(inds) % minibatch_size\n",
    "    if r:\n",
    "        yield tf.convert_to_tensor(inds[-r:], dtype=tf.int64)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff1968b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 84, 84, 4)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "state = tf.convert_to_tensor(env.reset()[0])\n",
    "state = tf.stack([state, state, state, state], axis=-1)\n",
    "state = tf.expand_dims(state, 0)\n",
    "\n",
    "ppo = PPO()\n",
    "\n",
    "start_time = time.time()\n",
    "# training\n",
    "for epoch in range(1):\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    dones = []\n",
    "    values = []\n",
    "    predictions = []\n",
    "\n",
    "    # collect data\n",
    "    for episode in range(EPISODE_LENGTH):\n",
    "        traj_info = ppo.model(state)\n",
    "\n",
    "        log_prob_old = traj_info['log_pi_a']\n",
    "        action = traj_info['a'].numpy()\n",
    "        value = traj_info['v'].numpy()\n",
    "\n",
    "        next_state, reward, done, _, _ = env.step(action[0])\n",
    "\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "        dones.append(done)\n",
    "        values.append(value)\n",
    "        predictions.append(log_prob_old)\n",
    "\n",
    "        next_state = tf.expand_dims(next_state, 0)\n",
    "        next_state = tf.stack([next_state, state[:, :, :, 0], state[:, :, :, 1], state[:, :, :, 2]], axis = -1)\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            state = tf.convert_to_tensor(env.reset()[0])\n",
    "            state = tf.stack([state, state, state, state], axis=-1)\n",
    "            state = tf.expand_dims(state, 0)\n",
    "            break\n",
    "\n",
    "print(state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13734156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "States Shape: (1776, 84, 84, 4)\n",
      "Actions Shape: (1776, 1)\n",
      "Rewards Shape: (1776,)\n",
      "Dones Shape: (1776,)\n",
      "Values Shape: (1776,)\n",
      "Predictions Shape: (1776, 1)\n"
     ]
    }
   ],
   "source": [
    "states = tf.squeeze(states, [1])\n",
    "states = tf.convert_to_tensor(states, dtype=tf.float32)\n",
    "actions = tf.convert_to_tensor(actions, dtype=tf.int32)\n",
    "rewards = tf.convert_to_tensor(rewards, dtype=tf.float32)\n",
    "dones = tf.convert_to_tensor(dones, dtype=tf.float32)\n",
    "values = tf.convert_to_tensor(values, dtype=tf.float32)\n",
    "predictions = tf.convert_to_tensor(predictions, dtype=tf.float32)\n",
    "\n",
    "print(f'States Shape: {states.shape}')\n",
    "print(f'Actions Shape: {actions.shape}')\n",
    "print(f'Rewards Shape: {rewards.shape}')\n",
    "print(f'Dones Shape: {dones.shape}')\n",
    "print(f'Values Shape: {values.shape}')\n",
    "print(f'Predictions Shape: {predictions.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4503fc8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1776, 1, 84, 84, 4)\n",
      "(1776, 1, 20, 20, 32)\n",
      "(1776, 1, 6, 6, 64)\n",
      "(1776, 1, 4, 4, 64)\n",
      "(1776, 1024)\n",
      "(1776, 512)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "states = tf.random.uniform(shape=(1776, 1, 84, 84, 4), minval=0, maxval=1, dtype=tf.float32)\n",
    "print(states.shape)\n",
    "# print(f'Random Tensor Shape: {states.shape}')\n",
    "\n",
    "# # states = tf.squeeze(states, [1])\n",
    "# traj_info = ppo.model(states, actions)\n",
    "\n",
    "\n",
    "states = tf.keras.layers.Conv2D(32, kernel_size=8, strides=4, activation='relu')(states)\n",
    "print(states.shape)\n",
    "states = tf.keras.layers.Conv2D(64, kernel_size=4, strides=3, activation='relu')(states)\n",
    "print(states.shape)\n",
    "states = tf.keras.layers.Conv2D(64, kernel_size=3, strides=1, activation='relu')(states)\n",
    "print(states.shape)\n",
    "\n",
    "flatten = tf.keras.layers.Flatten()(states)\n",
    "print(flatten.shape)\n",
    "fc = tf.keras.layers.Dense(512, activation='relu')(flatten)\n",
    "print(fc.shape)\n",
    "# self.conv2 = tf.keras.layers.Conv2D(64, kernel_size=4, strides=3, activation='relu')\n",
    "# self.conv3 = tf.keras.layers.Conv2D(64, kernel_size=3, strides=1, activation='relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e0d023c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1776,)\n"
     ]
    }
   ],
   "source": [
    "print(traj_info['v'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1c4e86ba",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[WinError 127] 找不到指定的程序。 Error loading \"D:\\Anaconda\\lib\\site-packages\\torch\\lib\\cublas64_11.dll\" or one of its dependencies.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15132\\3341719978.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# 在 PyTorch 中\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\torch\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    122\u001b[0m                 \u001b[0merr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWinError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlast_error\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m                 \u001b[0merr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrerror\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34mf' Error loading \"{dll}\" or one of its dependencies.'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 124\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    125\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mres\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m                 \u001b[0mis_loaded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 127] 找不到指定的程序。 Error loading \"D:\\Anaconda\\lib\\site-packages\\torch\\lib\\cublas64_11.dll\" or one of its dependencies."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import tensorflow as tf\n",
    "\n",
    "# 在 PyTorch 中\n",
    "a = torch.tensor([1, 2, 3])\n",
    "b = torch.tensor([4, 5, 6])\n",
    "c = torch.stack((a, b), dim=0)\n",
    "\n",
    "print(\"c in PyTorch:\")\n",
    "print(c)\n",
    "\n",
    "####################\n",
    "\n",
    "# 在 TensorFlow 中\n",
    "a_tf = tf.constant([1, 2, 3])\n",
    "b_tf = tf.constant([4, 5, 6])\n",
    "c_tf = tf.stack([a_tf, b_tf], axis=0)\n",
    "\n",
    "print(\"\\nc in TensorFlow:\")\n",
    "print(c_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f01df0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
